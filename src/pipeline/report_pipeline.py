"""
Main pipeline for processing and validating REDCap instrument data.

This module orchestrates the entire QC process, from fetching raw data to
generating final summary reports. It is designed to be run from the command
line via `cli/cli.py`.

The pipeline operates in several stages:
1.  **Configuration**: Loads settings from `QCConfig`.
2.  **Data Fetching**: Extracts data from the source using `pipeline.fetcher`.
3.  **Rule Loading**: Caches all JSON validation rules for the requested instruments.
4.  **Data Preparation**: Prepares instrument-specific dataframes for validation.
5.  **Validation**: Uses a standardized per-record validation approach through
    the QualityCheck system for consistent processing across all instrument types.
6.  **Reporting**: Generates several output files, including:
    - A final dataset of all identified errors.
    - Aggregate error counts per participant/event.
    - A tool status report indicating pass/fail for each instrument.
    - Detailed validation logs.

REFACTORING NOTE: The validation process has been standardized to use a single
unified approach instead of separate vectorized and dynamic validation paths.
This ensures consistency across all validation scenarios at the cost of some
performance optimization.
"""
import datetime
import json
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union, TYPE_CHECKING
from dataclasses import asdict

import numpy as np
import pandas as pd

if TYPE_CHECKING:
    from .context import ExportConfiguration, ReportConfiguration

from pipeline.config_manager import (
    QCConfig,
    get_dynamic_rule_instruments,
    get_discriminant_variable,
    is_dynamic_rule_instrument,
    get_rule_mappings,
    get_instrument_json_mapping,
    upload_ready_path
)
from pipeline.fetcher import RedcapETLPipeline
from nacc_form_validator.quality_check import QualityCheck
from pipeline.helpers import (
    build_complete_visits_df,
    build_detailed_validation_logs,
    build_variable_maps,
    load_rules_for_instruments,
    load_json_rules_for_instrument,
    prepare_instrument_data_cache,
    _preprocess_cast_types,
    load_dynamic_rules_for_instrument,
)
from pipeline.analytics import create_simplified_debug_info
from pipeline.reports import ReportFactory
from pipeline.context import ProcessingContext, ExportConfiguration, ReportConfiguration
from pipeline.schema_builder import build_cerberus_schema_for_instrument
from nacc_form_validator.utils import (
    convert_to_date,
    convert_to_datetime,
)

# Set up logging
import logging

logger = logging.getLogger(__name__)


def run_report_pipeline(config: QCConfig):
    """
    Main entry point for the QC report pipeline.

    This function orchestrates the entire process, from fetching data to
    generating the final reports.

    Args:
        config: The configuration object for the pipeline.
    """
    print("")
    print("="*80)
    print("STARTING QC REPORT PIPELINE")
    print("="*80)
    print("")

    # Create the main output directory based on run type, date, and time
    run_type_str = config.mode.replace("_", " ").title().replace(" ", "")
    current_datetime = datetime.datetime.now()
    date_tag = current_datetime.strftime("%d%b%Y").upper()
    time_tag = current_datetime.strftime("%H%M%S")
    run_dir_name = f"QC_{run_type_str}_{date_tag}_{time_tag}"

    output_path = Path(config.output_path) / run_dir_name
    output_path.mkdir(parents=True, exist_ok=True)

    (
        df_errors,
        df_logs,
        df_passed,
        all_records_df,
        complete_visits_df,
        detailed_logs_df,
    ) = process_instruments_etl(config, output_path, date_tag, time_tag)

    # Export results using unified ReportFactory
    processing_context = ProcessingContext(
        data_df=all_records_df,
        instrument_list=config.instruments,
        rules_cache={},  # Not needed for export
        primary_key_field=config.primary_key_field,
        config=config
    )
    
    export_config = ExportConfiguration(
        output_dir=output_path,
        date_tag=date_tag,
        time_tag=time_tag
    )
    
    report_config = ReportConfiguration(
        qc_run_by=config.user_initials or "N/A",
        primary_key_field=config.primary_key_field,
        instruments=config.instruments
    )
    
    report_factory = ReportFactory(processing_context)
    
    # Export all results with unified interface
    generated_files = report_factory.export_all_reports(
        df_errors=df_errors,
        df_logs=df_logs,
        df_passed=df_passed,
        all_records_df=all_records_df,
        complete_visits_df=complete_visits_df,
        detailed_validation_logs_df=detailed_logs_df,
        export_config=export_config,
        report_config=report_config
    )

    # Generate additional reports if data is available
    if not all_records_df.empty:
        # All reports are now generated by the unified ReportFactory above
        logger.info("Additional reports generated via unified ReportFactory")
    else:
        logger.warning("No processed records available - skipping aggregate reports")


def _collect_processed_records_info(
    df: pd.DataFrame, instrument: str, primary_key_field: str
) -> pd.DataFrame:
    """
    Collects key information about processed records for status reporting.

    This includes ptid, event name, instrument, and the discriminant
    variable for dynamic instruments.

    Args:
        df: The DataFrame of records processed for a single instrument.
        instrument: The name of the instrument.
        primary_key_field: The name of the primary key field.

    Returns:
        A DataFrame with essential information for status reports.
    """
    info_cols = [primary_key_field, "redcap_event_name"]
    rec = df[info_cols].copy()
    rec["instrument_name"] = instrument

    if is_dynamic_rule_instrument(instrument):
        discriminant_var = get_discriminant_variable(instrument)
        if discriminant_var in df.columns:
            rec[discriminant_var] = df[discriminant_var]
        else:
            rec[discriminant_var] = None

    return rec


# ───────────────────────────────── Data Validation with Json Rules ─────────────────────────────────

class ValidationEngine:
    """
    Optimized validation engine with caching and performance improvements.
    
    This maintains the per-record validation approach while optimizing
    supporting infrastructure for better performance.
    """
    
    def __init__(self):
        """Initialize validation engine with caches."""
        self._schema_cache: Dict[str, Any] = {}
        self._rule_cache: Dict[str, Any] = {}
        self._qc_cache: Dict[str, QualityCheck] = {}
    
    def _get_cache_key(self, instrument_name: str, variant: str = "") -> str:
        """Generate cache key for instrument and variant."""
        return f"{instrument_name}:{variant}" if variant else instrument_name
    
    def _get_cached_schema_and_rules(
        self,
        record: Dict[str, Any],
        instrument_name: str,
        default_rules: Dict[str, Any],
        primary_key_field: str,
    ) -> Tuple[QualityCheck, Dict[str, Any]]:
        """
        Get cached schema and rules with optimized lookups.
        
        This replaces _get_schema_and_rules_for_record with caching.
        """
        if is_dynamic_rule_instrument(instrument_name):
            discriminant_var = get_discriminant_variable(instrument_name)
            variant = str(record.get(discriminant_var, "")).upper()
            
            # Fallback to first variant if missing
            if not variant:
                rule_mappings = get_rule_mappings(instrument_name)
                variant = list(rule_mappings.keys())[0]
            
            cache_key = self._get_cache_key(instrument_name, variant)
        else:
            cache_key = self._get_cache_key(instrument_name)
            variant = ""
        
        # Check QualityCheck cache first
        if cache_key in self._qc_cache:
            qc = self._qc_cache[cache_key]
            rules = self._rule_cache.get(cache_key, default_rules)
            return qc, rules
        
        # Build schema and QualityCheck if not cached
        if is_dynamic_rule_instrument(instrument_name):
            # Get cached schema or build it
            if cache_key not in self._schema_cache:
                cerb_schema = build_cerberus_schema_for_instrument(
                    instrument_name, 
                    include_temporal_rules=False,
                    include_compatibility_rules=True
                )
                self._schema_cache[cache_key] = cerb_schema[variant]
                
                # Cache rules for this variant
                dynamic_rules = load_dynamic_rules_for_instrument(instrument_name)
                self._rule_cache[cache_key] = dynamic_rules[variant]
            
            sub_schema = self._schema_cache[cache_key]
            rules = self._rule_cache[cache_key]
            qc = QualityCheck(pk_field=primary_key_field, schema=sub_schema, datastore=None)
        else:
            # Standard instrument
            if cache_key not in self._schema_cache:
                cerb_schema = build_cerberus_schema_for_instrument(
                    instrument_name,
                    include_temporal_rules=False,
                    include_compatibility_rules=True
                )
                self._schema_cache[cache_key] = cerb_schema
                self._rule_cache[cache_key] = default_rules
            
            schema = self._schema_cache[cache_key]
            rules = self._rule_cache[cache_key]
            qc = QualityCheck(pk_field=primary_key_field, schema=schema, datastore=None)
        
        # Cache QualityCheck instance
        self._qc_cache[cache_key] = qc
        return qc, rules
    
    def validate_data_optimized(
        self,
        data: pd.DataFrame,
        validation_rules: Dict[str, Any],
        instrument_name: str,
        primary_key_field: str,
        event_name: Optional[str] = None,
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        Optimized validation with caching and reduced object creation.
        
        Maintains per-record processing while optimizing supporting infrastructure.
        """
        errors: List[Dict[str, Any]] = []
        validation_logs: List[Dict[str, Any]] = []
        passed_validations: List[Dict[str, Any]] = []

        df = data.copy()
        if event_name:
            df = df[df["redcap_event_name"] == event_name]

        # Pre-compute rule file mapping
        if is_dynamic_rule_instrument(instrument_name):
            instrument_json_mapping = get_rule_mappings(instrument_name)
        else:
            instrument_json_mapping = get_instrument_json_mapping()
            
        rule_files = instrument_json_mapping.get(instrument_name, [])
        rule_file_str = ",".join(rule_files) if isinstance(rule_files, list) else rule_files

        # --- Optimized per-record validation ---
        for _, row in df.iterrows():
            record = row.to_dict()

            qc, rules = self._get_cached_schema_and_rules(
                record, instrument_name, validation_rules, primary_key_field
            )

            result = qc.validate_record(record)

            # Optimized error logging with reduced object creation
            self._log_validation_results_optimized(
                record, rules, result.errors, qc, instrument_name,
                errors, validation_logs, passed_validations,
                primary_key_field, rule_file_str
            )

        return errors, validation_logs, passed_validations
    
    def _log_validation_results_optimized(
        self,
        record: Dict[str, Any],
        rules: Dict[str, Any],
        errs_dict: Dict[str, Any],
        qc: QualityCheck,
        instrument_name: str,
        errors: List[Dict[str, Any]],
        validation_logs: List[Dict[str, Any]],
        passed_validations: List[Dict[str, Any]],
        primary_key_field: str,
        rule_file_str: str,
    ):
        """
        Optimized validation result logging with reduced overhead.
        """
        pk_val = record.get(primary_key_field)
        event = record.get("redcap_event_name")

        # Process all variables in a single loop to reduce overhead
        for var, var_rules in rules.items():
            raw_val = record.get(var)
            str_val = str(raw_val) if raw_val is not None else ""
            interp_val = qc.validator.cast_record({var: str_val}).get(var, raw_val)

            expected_t = var_rules.get("type")
            fld_errs = errs_dict.get(var, [])
            err_msg = fld_errs[0] if fld_errs else None

            # Pre-computed common data
            base_log_data = {
                primary_key_field: pk_val,
                "variable": var,
                "redcap_event_name": event,
                "instrument_name": instrument_name,
            }

            # Always add to validation logs
            validation_logs.append({
                **base_log_data,
                "json_rule": json.dumps(var_rules),
                "rule_file": rule_file_str,
            })

            if err_msg:
                # Add to errors
                errors.append({
                    **base_log_data,
                    "current_value": interp_val,
                    "expected_value": expected_t,
                    "error": err_msg,
                })
            else:
                # Add to passed validations
                passed_validations.append({
                    **base_log_data,
                    "current_value": interp_val,
                    "json_rule": json.dumps(var_rules),
                    "rule_file": rule_file_str,
                })


# Global validation engine instance for reuse
_validation_engine = ValidationEngine()


def validate_data(
    data: pd.DataFrame,
    validation_rules: Dict[str, Any],
    instrument_name: str,
    primary_key_field: str,
    event_name: Optional[str] = None,
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    Validates a DataFrame of instrument data against a set of rules using a standardized process.

    This function now uses an optimized validation engine while maintaining the
    per-record validation approach required for ETL modularity and nacc_form_validator compatibility.

    Args:
        data: DataFrame containing the data for a specific instrument.
        validation_rules: A dictionary of JSON validation rules for the instrument.
        instrument_name: The name of the instrument being validated.
        primary_key_field: The name of the primary key field.
        event_name: If provided, filters the DataFrame to this specific event.

    Returns:
        A tuple containing:
        - A list of dictionaries, where each dictionary is a validation error.
        - A list of dictionaries representing detailed validation logs.
        - A list of dictionaries for validations that passed.
    """
    return _validation_engine.validate_data_optimized(
        data, validation_rules, instrument_name, primary_key_field, event_name
    )

    return errors, validation_logs, passed_validations

def _get_schema_and_rules_for_record(
    record: Dict[str, Any],
    cerb_schema: Dict[str, Any],
    instrument_name: str,
    default_rules: Dict[str, Any],
    primary_key_field: str,
) -> Tuple[QualityCheck, Dict[str, Any]]:
    """
    Selects the appropriate schema and rules for a record.
    
    Handles dynamic instruments by selecting a sub-schema and rules based on a
    discriminant variable's value in the record.

    Args:
        record: The data record (as a dictionary).
        cerb_schema: The top-level Cerberus schema for the instrument.
        instrument_name: The name of the instrument.
        default_rules: The default set of validation rules.

    Returns:
        A tuple containing the configured `QualityCheck` object and the
        applicable dictionary of validation rules.
    """
    if is_dynamic_rule_instrument(instrument_name):
        discriminant_var = get_discriminant_variable(instrument_name)
        variant = str(record.get(discriminant_var, "")).upper()
        
        # Fallback to the first defined variant if the key is missing or empty
        if not variant or variant not in cerb_schema:
            variant = list(cerb_schema.keys())[0]

        sub_schema = cerb_schema[variant]
        qc = QualityCheck(pk_field=primary_key_field, schema=sub_schema, datastore=None)
        # Load the specific rules for this variant
        rules = load_dynamic_rules_for_instrument(instrument_name)[variant]
    else:
        qc = QualityCheck(pk_field=primary_key_field, schema=cerb_schema, datastore=None)
        rules = default_rules

    return qc, rules


def _log_validation_results(
    record: Dict[str, Any],
    rules: Dict[str, Any],
    errs_dict: Dict[str, Any],
    qc: QualityCheck,
    instrument_name: str,
    errors: List[Dict[str, Any]],
    validation_logs: List[Dict[str, Any]],
    passed_validations: List[Dict[str, Any]],
    primary_key_field: str,
):
    """
    Logs the outcome of a validation check for each field in a record.

    For each variable in the rule set, this function records whether it passed
    or failed validation, creating detailed log entries.

    Args:
        record: The record that was validated.
        rules: The validation rules applied to the record.
        errs_dict: A dictionary of errors from the validator.
        qc: The `QualityCheck` instance used for validation.
        instrument_name: The name of the instrument.
        errors: The master list of errors to append to.
        validation_logs: The master list of validation logs to append to.
        passed_validations: The master list of passed validations to append to.
        primary_key_field: The name of the primary key field.
    """
    pk_val = record.get(primary_key_field)
    event = record.get("redcap_event_name")

    # Get the correct rule mapping based on instrument type
    if is_dynamic_rule_instrument(instrument_name):
        # For dynamic instruments, the rules are already specific, so we can get the mapping directly
        instrument_json_mapping = get_rule_mappings(instrument_name)
        # The variant is needed to get the correct file
        discriminant_var = get_discriminant_variable(instrument_name)
        variant = str(record.get(discriminant_var, "")).upper()
        files = instrument_json_mapping.get(variant, [])
    else:
        # For standard instruments, get the global mapping
        instrument_json_mapping = get_instrument_json_mapping()
        files = instrument_json_mapping.get(instrument_name, [])

    for var, var_rules in rules.items():
        raw_val = record.get(var)
        # Use the validator's type casting to get the interpreted value
        str_val = str(raw_val) if raw_val is not None else ""
        interp_val = qc.validator.cast_record({var: str_val}).get(var, raw_val)

        expected_t = var_rules.get("type")
        fld_errs = errs_dict.get(var, [])
        err_msg = fld_errs[0] if fld_errs else None

        rule_file = ",".join(files) if isinstance(files, list) else files

        validation_logs.append(
            {
                primary_key_field: pk_val,
                "variable": var,
                "json_rule": json.dumps(var_rules),
                "rule_file": rule_file,
                "redcap_event_name": event,
                "instrument_name": instrument_name,
            }
        )

        if err_msg:
            errors.append(
                {
                    primary_key_field: pk_val,
                    "redcap_event_name": event,
                    "instrument_name": instrument_name,
                    "variable": var,
                    "current_value": interp_val,
                    "expected_value": expected_t,
                    "error": err_msg,
                }
            )
        else:
            passed_validations.append(
                {
                    primary_key_field: pk_val,
                    "variable": var,
                    "current_value": interp_val,
                    "json_rule": json.dumps(var_rules),
                    "rule_file": rule_file,
                    "redcap_event_name": event,
                    "instrument_name": instrument_name,
                }
            )


# ──────────────────────────────────── Main ETL-Optimized Processing Function ─────────────────────────────────


def process_instruments_etl(
    config: QCConfig,
    output_path: Optional[Union[str, Path]] = None,
    date_tag: Optional[str] = None,
    time_tag: Optional[str] = None,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Orchestrates the ETL-optimized processing of instruments based on a config.

    Args:
        config: A `QCConfig` object containing all pipeline settings.
        output_path: Optional path to use for ETL output. If None, uses config.output_path.

    Returns:
        A tuple of DataFrames:
        (df_errors, df_logs, df_passed, all_records_df, complete_visits_df, detailed_validation_logs)
    """
    output_dir = Path(config.output_path)
    output_dir.mkdir(exist_ok=True)

    # Step 1: Load all necessary validation rules into a cache
    logger.info(f"Loading validation rules for {len(config.instruments)} instruments")
    rules_cache = load_rules_for_instruments(config.instruments)

    # Step 2: Fetch data using the modern ETL pipeline
    logger.info("Fetching data using modern RedcapETLPipeline.")
    try:
        pipeline = RedcapETLPipeline(config)
        etl_result = pipeline.run(output_path, date_tag, time_tag)
        data_df = etl_result.data
        logger.info(f"ETL pipeline completed: {etl_result.records_processed} records ready for processing "
                   f"(execution time: {etl_result.execution_time:.2f}s)")
    except Exception as e:
        logger.error(f"ETL pipeline failed: {e}", exc_info=True)
        raise RuntimeError(f"ETL data fetch failed: {e}") from e

    # Step 3: Apply complete visits filtering BEFORE building the instrument data cache
    complete_visits_df = pd.DataFrame()
    if config.mode == "complete_visits" and not data_df.empty:
        complete_visits_df, _ = build_complete_visits_df(data_df, config.instruments)
        
        # Filter data_df to only include records from complete visits
        if not complete_visits_df.empty:
            # Create a mask for complete visits
            complete_visits_mask = data_df.set_index(['ptid', 'redcap_event_name']).index.isin(
                complete_visits_df.set_index(['ptid', 'redcap_event_name']).index
            )
            data_df = data_df[complete_visits_mask].copy()
            logger.debug(f"Filtered to {len(data_df)} records from {len(complete_visits_df)} complete visits")
        else:
            logger.warning("No complete visits found - no data will be processed")
            data_df = pd.DataFrame()  # Empty the dataframe if no complete visits

    # Step 4: Prepare data and mappings with filtered data
    if not data_df.empty:
        # Use new simplified analytics for debug information
        debug_info = create_simplified_debug_info(data_df, config.instruments, rules_cache, "summary")
        
        if 'summary' in debug_info:
            coverage = debug_info['summary'].get('overall_coverage', 'N/A')
            logger.debug(f"Variable mapping analysis: {coverage} coverage")
        
        # Log any missing variables
        if 'missing_variables' in debug_info:
            for instrument, missing_vars in debug_info['missing_variables'].items():
                if missing_vars:
                    logger.warning(f"Missing variables for {instrument}: {missing_vars[:5]}{'...' if len(missing_vars) > 5 else ''}")

    _, instrument_variable_map = build_variable_maps(config.instruments, rules_cache)
    instrument_data_cache = {}
    if not data_df.empty:
        instrument_data_cache = prepare_instrument_data_cache(
            data_df,
            config.instruments,
            instrument_variable_map,
            rules_cache,
            config.primary_key_field,
        )

    logger.info("Starting validation processing...")

    all_errors: List[Dict[str, Any]] = []
    all_logs: List[Dict[str, Any]] = []
    all_passed: List[Dict[str, Any]] = []
    records_for_status: List[pd.DataFrame] = []
    detailed_validation_logs: List[Dict[str, Any]] = []

    # Step 5: Process each instrument
    for i, instrument in enumerate(config.instruments, 1):
        logger.info(f"Processing {instrument} ({i}/{len(config.instruments)})")
        df = instrument_data_cache.get(instrument, pd.DataFrame())
        if df.empty:
            logger.warning(f"No data available for instrument '{instrument}' after ETL filtering.")
            continue
        
        # Build screening logs based on instrument completeness
        logs_for_this = build_detailed_validation_logs(
            df, instrument, primary_key_field=config.primary_key_field
        )
        detailed_validation_logs.extend(logs_for_this)
        
        rules = rules_cache[instrument]
        df = _preprocess_cast_types(df, rules)
        errors, logs, passed_records = validate_data(
            df, rules, instrument_name=instrument, primary_key_field=config.primary_key_field
        )
        
        all_errors.extend(errors)
        all_logs.extend(logs)
        all_passed.extend(passed_records)
        records_for_status.append(
            _collect_processed_records_info(
                df, instrument, primary_key_field=config.primary_key_field
            )
        )

    df_errors = pd.DataFrame(all_errors) if all_errors else pd.DataFrame()
    df_logs = pd.DataFrame(all_logs) if all_logs else pd.DataFrame()
    df_passed = pd.DataFrame(all_passed) if all_passed else pd.DataFrame()
    
    all_records_df = pd.DataFrame()
    if records_for_status:
        all_records_df = pd.concat(records_for_status, ignore_index=True).drop_duplicates(
            subset=[config.primary_key_field, "redcap_event_name", "instrument_name"]
        )

    logger.info(
        f"VALIDATION SUMMARY: {len(df_errors)} errors found across {len(config.instruments)} instruments"
    )

    print("")
    print("="*80)
    print("ETL INSTRUMENT PROCESSING COMPLETED SUCCESSFULLY")
    print("="*80)

    return (
        df_errors,
        df_logs,
        df_passed,
        all_records_df,
        complete_visits_df,
        pd.DataFrame(detailed_validation_logs) if detailed_validation_logs else pd.DataFrame(),
    )

# ───────────────────────────────── Export Functions ─────────────────────────────────
# 
# DEPRECATED FUNCTIONS HAVE BEEN MOVED
# 
# The following functions have been moved to pipeline/deprecated.py:
# - export_results_to_csv()
# - generate_aggregate_error_count_report() 
# - generate_tool_status_reports()
# - debug_variable_mapping()
#
# These functions are deprecated and will be removed in version 2.0.0.
# Use the new ReportFactory and DataQualityAnalyzer classes instead:
#
# OLD: export_results_to_csv(df_errors, df_logs, ...)
# NEW: ReportFactory(context).export_all_reports(df_errors, df_logs, ..., config, report_config)
#
# OLD: debug_variable_mapping(data_df, instruments, rules_cache)
# NEW: DataQualityAnalyzer().analyze_coverage(data_df, instruments, rules_cache)
#
# DEPRECATED FUNCTIONS HAVE BEEN REMOVED
# 
# The following deprecated functions have been completely removed:
# - export_results_to_csv() → Use ReportFactory.export_all_reports()
# - generate_aggregate_error_count_report() → Use ReportFactory.generate_aggregate_error_report()
# - generate_tool_status_reports() → Use ReportFactory.generate_status_report()
# - debug_variable_mapping() → Use create_simplified_debug_info()
# - generate_enhanced_summary_report() → Use ReportFactory.generate_status_report()
#
# Migration guide available in documentation.
#
# ─────────────────────────────────────────────────────────────────────────────────────
